{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXwUV4jLT1MR"
   },
   "source": [
    "__FRONT_MATTER__\n",
    "\n",
    "---\n",
    "title: \"Using Lyrics to Predict Genre\"\n",
    "\n",
    "we'll implement naive Bayes, to predict a genre given some lyrics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNKlTcQ4T1MV"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_esKBFcXT1MV"
   },
   "source": [
    "## Contents\n",
    "\n",
    "• [Getting the Data](#Getting-the-Data)<br/>\n",
    "• [Loading the Data](#Loading-the-Data)<br/>\n",
    "• [Splitting the Data](#Splitting-the-Data)<br/>\n",
    "• [Training the Model](#Training-the-Model)<br/>\n",
    "• [Top Hip Hop Songs](#Top-Hip-Hop-Songs)<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdfdNS-0T1MW"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu48h6JhT1MX"
   },
   "source": [
    "Majority of the people are not aware of the various Genres of music. And even if they are are it very tough to actually determine the genre of a particular song.\n",
    "\n",
    "We aim to Solve this issue by creating an app in which the user simply has to paste a part of the song lyrics , and they get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Impact\n",
    "if combined with audio recognition aswell..it can be used in song Recommendation systems .By Showing songs of similar genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems faced\n",
    "getting the data in the desired format was a task .\n",
    "secondly selecting which all genres to consider for this project. cause many genres are very similar to each other which may cause problems with the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr1df8EJT1MX"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hTydlliVT1MY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/Users/yashsawant/Downloads/cypher-master/notebooks/lyrics1.csv')\n",
    "\n",
    "df['ranker_genre'] = np.where(\n",
    "    (df['ranker_genre'] == 'screamo')|\n",
    "    (df['ranker_genre'] == 'punk rock')|\n",
    "    (df['ranker_genre'] == 'heavy metal'), \n",
    "    'alt rock', \n",
    "    df['ranker_genre']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlF0Fxw4T1MY"
   },
   "source": [
    "The data is available as one lyric per row. To train our classifier, we'll need to transform it into one *song* per row. We'll also go ahead and convert the data to lowercase with `.apply(lambda x: x.lower())`. To do that, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rZ7BxeH1T1MY"
   },
   "outputs": [],
   "source": [
    "group = ['song', 'year', 'album', 'genre', 'artist', 'ranker_genre']\n",
    "lyrics_by_song = df.sort_values(group)\\\n",
    "        .groupby(group).lyric\\\n",
    "        .apply(' '.join)\\\n",
    "        .apply(lambda x: x.lower())\\\n",
    "        .reset_index(name='lyric')\n",
    "\n",
    "lyrics_by_song[\"lyric\"] = lyrics_by_song['lyric'].str.replace(r'[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "CKho2cirT1MZ"
   },
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhE5u5iGT1MZ"
   },
   "source": [
    "Next we'll split our data into a training set and a testing set using only Country, Alt Rock, and Hip Hop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SQy4mIPlT1MZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "genres = [\n",
    "    'Country', 'alt rock', 'Hip Hop',\n",
    "]\n",
    "\n",
    "LYRIC_LEN = 400 # each song has to be > 400 characters\n",
    "N = 6000 # number of records to pull from each genre\n",
    "RANDOM_SEED = 200 # random seed to make results repeatable\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "for genre in genres: # loop over each genre\n",
    "    subset = lyrics_by_song[ # create a subset \n",
    "        (lyrics_by_song.ranker_genre==genre) & \n",
    "        (lyrics_by_song.lyric.str.len() > LYRIC_LEN)\n",
    "    ]\n",
    "    train_set = subset.sample(n=N, random_state=RANDOM_SEED)\n",
    "    test_set = subset.drop(train_set.index)\n",
    "    train_df = train_df.append(train_set) # append subsets to the master sets\n",
    "    test_df = test_df.append(test_set)\n",
    "    \n",
    "train_df = shuffle(train_df)\n",
    "test_df = shuffle(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-ZPY_8GT1Ma"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNC956IWT1Ma"
   },
   "source": [
    "Next, we'll train a model using word frequencies and `sklearn`'s `CountVectorizer`. The `CountVectorizer` is a quick and dirty way to train a language model by using simple word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXaqqqYhT1Ma",
    "outputId": "88bf8bcd-9544-4de7-d093-b3ec426ba092"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8803284361531444"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', CountVectorizer()),\n",
    "     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyric, train_df.ranker_genre)  \n",
    "\n",
    "# score our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyric)\n",
    "np.mean(predicted == test_df.ranker_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEAX2e7ET1Mb"
   },
   "source": [
    "\n",
    "Word frequencies work fine here, but let's see if we can get a better model by using the `TfidfVectorizer`.\n",
    "\n",
    "So let's train a model using `tf-idf` scores as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHrTyMjaT1Mb",
    "outputId": "bef4a1b4-f42c-4a00-8e91-583bfbebcd69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8813351370056941"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyric, train_df.ranker_genre)  \n",
    "\n",
    "# score our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyric)\n",
    "np.mean(predicted == test_df.ranker_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JXpfkY_T1Mb"
   },
   "source": [
    "tuning a few hyperparameters, lemmatizing our data, customizing our tokenizer a bit, and filtering our words with `nltk`'s builtin stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6bp7OBST1Mc",
    "outputId": "c5fdeafb-6065-4e0f-945e-9c7e8b0f99f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsawant/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'doe', 'ha', 'might', 'must', 'need', 'sha', 'wa', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8740680152263504"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop = list(set(stopwords.words('english'))) # stopwords\n",
    "wnl = WordNetLemmatizer() # lemmatizer\n",
    "\n",
    "def tokenizer(x): # custom tokenizer\n",
    "    return (\n",
    "        wnl.lemmatize(w) \n",
    "        for w in word_tokenize(x) \n",
    "        if len(w) > 2 and w.isalnum() # only words that are > 2 characters\n",
    "    )                                 # and is alpha-numeric\n",
    "\n",
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer(\n",
    "        ngram_range=(1, 2), # include bigrams\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop,\n",
    "        max_df=0.4, # ignore terms that appear in more than 40% of documents\n",
    "        min_df=4)), # ignore terms that appear in less than 4 documents\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyric, train_df.ranker_genre)  \n",
    "\n",
    "# score our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyric)\n",
    "np.mean(predicted == test_df.ranker_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "xgkMQBo9T1Mc"
   },
   "source": [
    "Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "syu84-qYT1Md",
    "outputId": "3cdd6c5e-6cab-4a18-a76b-5083a059dce6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "mat = confusion_matrix(test_df.ranker_genre, predicted)\n",
    "sns.heatmap(\n",
    "    mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "    xticklabels=genres, \n",
    "    yticklabels=genres\n",
    ")\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "pETq9odnT1Md"
   },
   "source": [
    "Given this confusion matrix, we can calculate precision, recall, and f-score.\n",
    "\n",
    "<b>Recall</b> is the ability of the classifier to find all the positive results. That is, to clasify a rap song *as* a rap song. \n",
    "\n",
    "<b>Precision</b> is the ability of the classifier to not label a negative result as a positive one. That is, to not classify a country song as a rap song.\n",
    "\n",
    "<b>F-score</b> is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of precision and recall.\n",
    "\n",
    "To compute recall, precision, and f-score, we'll use `precision_recall_fscore_support` from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "837w1jAdT1Md",
    "outputId": "dbb62c48-2d1d-4002-9d42-2676e3890f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTRY_precision: 0.9411030826012372\n",
      "COUNTRY_recall: 0.8900250725136424\n",
      "COUNTRY_fscore: 0.9148516852797007\n",
      "COUNTRY_support: 20341\n",
      "\n",
      "ALT ROCK_precision: 0.26256983240223464\n",
      "ALT ROCK_recall: 0.9454022988505747\n",
      "ALT ROCK_fscore: 0.41099312929419113\n",
      "ALT ROCK_support: 348\n",
      "\n",
      "HIP HOP_precision: 0.8277418783747897\n",
      "HIP HOP_recall: 0.8425842494143089\n",
      "HIP HOP_fscore: 0.8350971198928332\n",
      "HIP HOP_support: 11098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(test_df.ranker_genre, predicted)\n",
    "\n",
    "for n,genre in enumerate(genres):\n",
    "    genre = genre.upper()\n",
    "    print(genre+'_precision: {}'.format(precision[n]))\n",
    "    print(genre+'_recall: {}'.format(recall[n]))\n",
    "    print(genre+'_fscore: {}'.format(fscore[n]))\n",
    "    print(genre+'_support: {}'.format(support[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACTSP21gT1Me"
   },
   "source": [
    "<b>Support</b> is the number of each class in the actual true set. And the first thing I notice is that there aren't many alt rock songs being scored. Adding more alt rock songs could possibly improve our model. \n",
    "\n",
    "We do a good job all around on classifying hip hop and country songs. For alt rock songs, the recall score is great; that is, when it's actually an alt rock song, the model classifies it as an alt rock song 94% of the time. But, as we can see from our alt rock precision score and confusion matrix, the model classifies many hip hop songs as alt rock (963, to be exact), which is the main reason this score is so low.\n",
    "\n",
    "Let's throw some new data at our model and see how well it does predicting what genre these lyrics belong to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MUTsFu1T1Me",
    "outputId": "303e5164-d9c4-4de6-bd2a-978775c3c8ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Country', 'Hip Hop', 'Country', 'alt rock', 'Country', 'Hip Hop',\n",
       "       'Country', 'alt rock', 'alt rock', 'Hip Hop'], dtype='<U8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(\n",
    "    [\n",
    "        \"i stand for the red white and blue\",\n",
    "        \"flow so smooth they say i rap in cursive\", #bars *insert fire emoji*\n",
    "        \"take my heart and carve it out\",\n",
    "        \"there is no end to the madness\",\n",
    "        \"sitting on my front porch drinking sweet tea\",\n",
    "        \"sitting on my front porch sippin on cognac\",\n",
    "        \"dog died and my pick up truck wont start\",\n",
    "        \"im invisible and the drugs wont help\",\n",
    "        \"i hope you choke in your sleep thinking of me\",\n",
    "        \"i wonder what genre a song about data science and naive bayes and hyper parameters and maybe a little scatter plots would be\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjfzjDbbT1Me"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Country'], dtype='<U8')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = input()\n",
    "text_clf.predict([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out= open(\"genrepredict.pkl\",\"wb\")\n",
    "pickle.dump(text_clf, pickle_out)\n",
    "pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = open('genrepredict.pkl','rb')\n",
    "classifier = pickle.load(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsawant/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'doe', 'ha', 'might', 'must', 'need', 'sha', 'wa', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['alt rock'], dtype='<U8')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict([\"hey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-1c8b826b6769>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-1c8b826b6769>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    streamlit run /Users/yashsawant/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 18:43:26.115 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/yashsawant/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x7fc06208e410>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Headings for Web Application\n",
    "st.title(\"Genre Classification of songs Using Lyrics\")\n",
    "st.subheader(\"What type of NLP service would you like to use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = st.text_input('Enter song lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x7fc06208e410>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.header(\"Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_clf.predict([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-c7a1b683aa76>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-c7a1b683aa76>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    streamlit hello\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "naive+bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
